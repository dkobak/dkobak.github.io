<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="icon" href="favicon.ico">
    
        <title>Transformers, Large Language Models, and their use in Physics</title>
        <style>
            html{margin:0; padding:0}
            body{max-width:900px; margin:40px auto; padding:0 10px; font-size:18px; font-family: sans-serif; line-height:1.3; color:#444}
            li{margin-bottom:0.3em}
            hr{margin-top:2em; height:1px; border:none; color:#444; background-color:#444;}
            @media (prefers-color-scheme: dark){
                body{color:#c9d1d9; background:#0d1117}
                a:link{color:#58a6ff}
                a:visited{color:#8e96f0}
            }
        </style>
    </head>
    

    <body>
        <h1>Transformers, large language models, and their use in physics</h1>
        
        <p>This seminar is offered for the MSc students of the Physics and Astronomy department of the Heidelberg university (<i>Computational Physics</i> specialisation). The seminar (SWS: 2; Leistungspunkte: 6; <a href="https://lsf.uni-heidelberg.de/qisserver/rds?state=verpublish&status=init&vmfile=no&publishid=392688&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung">LSF</a>) will be held for the first time in the winter semester 2023/24. The language of the seminar is English. To get credits, participants have to prepare one polished presentation and later submit a written report.
        
        <p><b>The teacher:</b> <a href="../../">Dr Dmitry Kobak</a> is a group leader at the TÃ¼bingen University working on dimensionality reduction and self-supervised learning. During the winter semester 2023/24, he is a visiting professor in Heidelberg. 
        
        <p><b>When/where:</b> Thursday 14:15&ndash;15:45. INF 205, SR 11.

        <hr>
        
        <h2>Schedule</h2>
        <ul>
            <li>Nov 9: Andrij Karpathy, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let's build GPT</a>, part 1
            <li>Nov 16: Andrij Karpathy, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let's build GPT</a>, part 2
            <li>Nov 23: Subbarao Kambhampati, <a href="https://www.youtube.com/watch?v=LQhcxDSMqtE">Avenging Polanyi's Revenge</a>
            <li>Nov 30: Jyot Makadiya:  Evaluating cognitive abilities of LLMs
            <li>Dec 7: Ken von Buenau: Training data memorization in LLMs
            <li>Dec 14: Pit Neitemeier: Mechanistic understanding of LLMs: sparse coding
            <li>Jan 11: Johnly Joshy: Mechanistic understanding of LLMs: attention circuits
            <li>Jan 18: Aditya Rastogi: ViT, CLIP, and MedCLIP
            <li>Jan 25: Philip Velie: Transformers for jet tagging at LHC
            <li>Feb 1: Johannes Schmidt: Transformers for quantum chemistry
        </ul>



        
        <hr>
        
        <h2>Some possible topics</h2>
        
        <ul>
        <li><b>Introduction</b>
            <ul>
            <li>Transformer architecture
                <ul>
                <li>Bloehm, <a href="https://peterbloem.nl/blog/transformers">Transformers from scratch</a>
                <li>Karpathy, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let's build GPT: from scratch, in code, spelled out</a>
                </ul>
            </ul>

        <li><b>LLM intelligence</b>
            <ul>
            <li>Emergent abilities of LLMs 
                <ul>
                <li>Srivastava et al, <a href="https://arxiv.org/abs/2206.04615">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</a>
                <li>Wei et al, <a href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models</a>
                </ul>
            <li>Sparks of AGI
                <ul>
                <li>Bubeck et al, <a href="https://arxiv.org/abs/2303.12712">Sparks of Artificial General Intelligence: Early experiments with GPT-4</a>
                </ul>
            <li>GPT-4 cannot self-critique / plan
                <ul>
                <li>Stechly et al., <a href="https://arxiv.org/abs/2310.12397">GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems</a>
                <li>Valmeekam et al, <a href="https://arxiv.org/abs/2310.08118">Can Large Language Models Really Improve by Self-critiquing Their Own Plans?</a>
                </ul>
            <li>Systematic cognitive evaluation
                <ul>
                <li>Mommenejad et al., <a href="https://arxiv.org/abs/2309.15129">Evaluating Cognitive Maps and Planning in Large Language Models with CogEval</a>
                </ul>
            <li>World models debate
                <ul>
                <li>Gurnee & Tegmark, <a href="https://arxiv.org/abs/2310.02207">Language Models Represent Space and Time</a> (cf. Novembre et al, <a href="https://www.nature.com/articles/nature07331">Genes mirror geography within Europe</a> and Mikolov et al, <a href="https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html">Distributed Representations of Words and Phrases and their Compositionality</a>)
                </ul>
            <li>Training data memorization	
                <ul>
                <li>McCoy et al, <a href="https://arxiv.org/abs/2309.13638">Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve</a> 
                <li>Wu et al, <a href="https://arxiv.org/abs/2307.02477">Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks</a>
                <li>Razeghi et al, <a href="https://arxiv.org/abs/2202.07206">Impact of Pretraining Term Frequencies on Few-Shot Reasoning</a>
                <li>Briakou et al, <a href="https://arxiv.org/abs/2305.10266">Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability</a>
                </ul>
            <li>In-context learning
                <ul>
                <li>Lu et al, <a href="https://arxiv.org/abs/2309.01809">Are Emergent Abilities in Large Language Models just In-Context Learning?</a>
                </ul>
            </ul>
        <li><b>Understanding LLMs</b>
            <ul>
            <li>Mechanistic understanding via sparse coding
                <ul>
                <li>Bricken et al, <a href="https://transformer-circuits.pub/2023/monosemantic-features/">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a>
                <li>Cunningham et al, <a href="https://arxiv.org/abs/2309.08600">Sparse Autoencoders Find Highly Interpretable Features in Language Models</a>
                </ul>
            <li>Mechanistic understanding of attention layers
                <ul>
                <li>Elhage et al, <a href="https://transformer-circuits.pub/2021/framework/">A Mathematical Framework for Transformer Circuits</a>
                </ul>
            <li>Grammar learning
                <ul>
                <li>Allen-Zhu & Li, <a href="https://arxiv.org/abs/2305.13673">Physics of Language Models: Part 1, Context-Free Grammar</a>
                </ul>
            </ul>
        <li><b>Other LLM topics</b>
            <ul>
            <li>Tiny LLMs
                <ul>
                <li>Eldan & Li, <a href="https://arxiv.org/abs/2305.07759">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a>
                </ul>
            <li>Time series modelling using LLMs
                <ul>
                <li>Gruver et al, <a href="https://arxiv.org/abs/2310.07820">Large Language Models Are Zero-Shot Time Series Forecasters</a>
                </ul>
            <li>Philosophy
                <ul>
                <li>Bottou & Schoelkopf, <a href="https://arxiv.org/abs/2310.01425">Borges and AI</a>
                </ul>
            </ul>
        <li><b>Transformers in other domains</b>
            <ul>
            <li>Vision transformers
                <ul>
                <li>Dosovitstkiy et al, <a href="https://openreview.net/forum?id=YicbFdNTTy">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>     
                <li>Darcet et al, <a href="https://arxiv.org/abs/2309.16588">Vision Transformers Need Registers</a>
                </ul>
            <li>Transformers for LHC data analysis
                <ul>
                <li>Qu et al, <a href="https://proceedings.mlr.press/v162/qu22b.html">Particle Transformer for Jet Tagging</a>
                </ul>
            <li>Transformers in quantum chemistry
                <ul>
                <li>Zhang et al, <a href="https://arxiv.org/abs/2309.16578">M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning</a>
                </ul>
            </ul>
        </ul>      
    </body>
</html>

