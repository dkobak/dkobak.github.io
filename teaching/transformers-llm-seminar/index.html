<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="icon" href="favicon.ico">
    
        <title>Transformers, Large Language Models, and their use in Physics</title>
        <style>
            html{margin:0; padding:0}
            body{max-width:900px; margin:40px auto; padding:0 10px; font-size:18px; font-family: sans-serif; line-height:1.3; color:#444}
            li{margin-bottom:0.3em}
            hr{margin-top:2em; height:1px; border:none; color:#444; background-color:#444;}
            @media (prefers-color-scheme: dark){
                body{color:#c9d1d9; background:#0d1117}
                a:link{color:#58a6ff}
                a:visited{color:#8e96f0}
            }
        </style>
    </head>
    

    <body>
        <h1>Transformers, large language models, and their use in physics</h1>
        
        <p>This seminar is offered for the MSc students of the Physics and Astronomy department of the Heidelberg university (<i>Computational Physics</i> specialisation). The seminar (SWS: 2, Leistungspunkte: 6) will be held for the first time in the winter semester 2023/24. The language of the seminar is English.

        <p>The seminar is in the process of getting approved for MSc students from the Computer Science department (but CS students can only get 2 credits for it; they do not need, however, to prepare a written report).
        
        <p><b>The teacher:</b> <a href="../../">Dr Dmitry Kobak</a> is a group leader at the Tübingen University working on dimensionality reduction and self-supervised learning. During the winter semester 23/24, he is a visiting professor in Heidelberg. 
        
        <p><b>When/where:</b> Thursday 14:00&ndash;15:45. INF 205, SR 11.
        
        <p><b>Links:</b> <a href="https://lsf.uni-heidelberg.de/qisserver/rds?state=verpublish&status=init&vmfile=no&publishid=392688&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung">LSF</a>. Please register at <a href="https://uebungen.physik.uni-heidelberg.de/uebungen/">Übungsgruppensystem</a>.

        
        <hr>
        
        <p>The seminar will focus on transformers: LLMs, vision transformers, and some applications of transformers in physics. We will discuss the architecture of transformers and language models such GPT, the emergent language skills of modern LLMs, and their shortcomings. We will also cover vision transformers. Finally, we will discuss some applications of transformers in physics, such as e.g. in LHC data analysis and in density functional theory. The exact list of topics depends on the participants, and will be posted here later.
        
        <p>All participants will have to prepare one polished presentation and later submit a written report.
        
        <h2>Possible topics</h2>
        
        <ul>
        <li><b>Introduction</b>
            <ul>
            <li>Transformer architecture
                <ul>
                <li>Bloehm, <a href="https://peterbloem.nl/blog/transformers">Transformers from scratch</a>
                <li>Karpathy, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let's build GPT: from scratch, in code, spelled out</a>
                </ul>
            </ul>

        <li><b>LLM intelligence</b>
            <ul>
            <li>Emergent abilities of LLMs 
                <ul>
                <li>Srivastava et al, <a href="https://arxiv.org/abs/2206.04615">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</a>
                <li>Wei et al, <a href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models</a>
                </ul>
            <li>Sparks of AGI
                <ul>
                <li>Bubeck et al, <a href="https://arxiv.org/abs/2303.12712">Sparks of Artificial General Intelligence: Early experiments with GPT-4</a>
                </ul>
            <li>GPT-4 cannot self-critique / plan
                <ul>
                <li>Stechly et al., <a href="https://arxiv.org/abs/2310.12397">GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems</a>
                <li>Valmeekam et al, <a href="https://arxiv.org/abs/2310.08118">Can Large Language Models Really Improve by Self-critiquing Their Own Plans?</a>
                </ul>
            <li>Systematic cognitive evaluation
                <ul>
                <li>Mommenejad et al., <a href="https://arxiv.org/abs/2309.15129">Evaluating Cognitive Maps and Planning in Large Language Models with CogEval</a>
                </ul>
            <li>World models debate
                <ul>
                <li>Gurnee & Tegmark, <a href="https://arxiv.org/abs/2310.02207">Language Models Represent Space and Time</a> (cf. Novembre et al, <a href="https://www.nature.com/articles/nature07331">Genes mirror geography within Europe</a> and Mikolov et al, <a href="https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html">Distributed Representations of Words and Phrases and their Compositionality</a>)
                </ul>
            <li>Training data memorization	
                <ul>
                <li>McCoy et al, <a href="https://arxiv.org/abs/2309.13638">Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve</a> 
                <li>Wu et al, <a href="https://arxiv.org/abs/2307.02477">Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks</a>
                <li>Razeghi et al, <a href="https://arxiv.org/abs/2202.07206">Impact of Pretraining Term Frequencies on Few-Shot Reasoning</a>
                <li>Briakou et al, <a href="https://arxiv.org/abs/2305.10266">Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability</a>
                </ul>
            <li>In-context learning
                <ul>
                <li>Lu et al, <a href="https://arxiv.org/abs/2309.01809">Are Emergent Abilities in Large Language Models just In-Context Learning?</a>
                </ul>
            </ul>
        <li><b>Understanding LLMs</b>
            <ul>
            <li>Mechanistic understanding via sparse coding
                <ul>
                <li>Bricken et al, <a href="https://transformer-circuits.pub/2023/monosemantic-features/">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a>
                <li>Cunningham et al, <a href="https://arxiv.org/abs/2309.08600">Sparse Autoencoders Find Highly Interpretable Features in Language Models</a>
                </ul>
            <li>Mechanistic understanding of attention layers
                <ul>
                <li>Elhage et al, <a href="https://transformer-circuits.pub/2021/framework/">A Mathematical Framework for Transformer Circuits</a>
                </ul>
            <li>Grammar learning
                <ul>
                <li>Allen-Zhu & Li, <a href="https://arxiv.org/abs/2305.13673">Physics of Language Models: Part 1, Context-Free Grammar</a>
                </ul>
            </ul>
        <li><b>Other LLM topics</b>
            <ul>
            <li>Tiny LLMs
                <ul>
                <li>Eldan & Li, <a href="https://arxiv.org/abs/2305.07759">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a>
                </ul>
            <li>Time series modelling using LLMs
                <ul>
                <li>Gruver et al, <a href="https://arxiv.org/abs/2310.07820">Large Language Models Are Zero-Shot Time Series Forecasters</a>
                </ul>
            <li>Philosophy
                <ul>
                <li>Bottou & Schoelkopf, <a href="https://arxiv.org/abs/2310.01425">Borges and AI</a>
                </ul>
            </ul>
        <li><b>Transformers in other domains</b>
            <ul>
            <li>Vision transformers
                <ul>
                <li>Dosovitstkiy et al, <a href="https://openreview.net/forum?id=YicbFdNTTy">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>     
                <li>Darcet et al, <a href="https://arxiv.org/abs/2309.16588">Vision Transformers Need Registers</a>
                </ul>
            <li>Transformers for LHC data analysis
                <ul>
                <li>Qu et al, <a href="https://proceedings.mlr.press/v162/qu22b.html">Particle Transformer for Jet Tagging</a>
                </ul>
            <li>Transformers in quantum chemistry
                <ul>
                <li>Zhang et al, <a href="https://arxiv.org/abs/2309.16578">M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning</a>
                </ul>
            </ul>
        </ul>      
    </body>
</html>

