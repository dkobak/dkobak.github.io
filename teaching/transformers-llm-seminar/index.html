<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="icon" href="favicon.ico">
    
        <title>Transformers, Large Language Models, and their use in Physics</title>
        <style>
            html{margin:0; padding:0}
            body{max-width:900px; margin:40px auto; padding:0 10px; font-size:18px; font-family: sans-serif; line-height:1.3; color:#444}
            li{margin-bottom:0.3em}
            hr{margin-top:2em; height:1px; border:none; color:#444; background-color:#444;}
            @media (prefers-color-scheme: dark){
                body{color:#c9d1d9; background:#0d1117}
                a:link{color:#58a6ff}
                a:visited{color:#8e96f0}
            }
        </style>
    </head>
    

    <body>
        <h1>Transformers, large language models, and their use in physics</h1>
        
        <p>This seminar is offered for the MSc students of the Physics and Astronomy department of the Heidelberg university (<i>Computational Physics</i> specialisation). The seminar (SWS: 2, Leistungspunkte: 6) will be held for the first time in the winter semester 2023/24. The language of the seminar is English.
        
        <p><b>The teacher:</b> <a href="../../">Dr Dmitry Kobak</a> is a group leader at the Tübingen University working on dimensionality reduction and self-supervised learning. During the winter semester 23/24, he is a visiting professor in Heidelberg. 
        
        <p><b>When/where:</b> Thursday 14:00&ndash;15:45. INF 205, SR 11.
        
        <p><b>Links:</b> <a href="https://lsf.uni-heidelberg.de/qisserver/rds?state=verpublish&status=init&vmfile=no&publishid=392688&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung">LSF</a>. Bitte beim <a href="https://uebungen.physik.uni-heidelberg.de/uebungen/">Übungsgruppensystem</a> anmelden.

        
        <hr>
        
        <p>The seminar will focus on transformers: LLMs, vision transformers, and some applications of transformers in physics. We will discuss the architecture of transformers and language models such GPT, the emergent language skills of modern LLMs, and their shortcomings. We will also cover vision transformers. Finally, we will discuss some applications of transformers in physics, such as e.g. in LHC data analysis and in density functional theory. The exact list of topics depends on the participants, and will be posted here later.
        
        <p>Every week we will be discussing one paper. Everybody will be required to read it in advance and submit a short written summary. Everybody will have to prepare one polished presentation and later submit a written report.
    </body>
</html>

